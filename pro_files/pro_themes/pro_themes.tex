\documentclass[10pt, a4paper]{extarticle}
\setlength{\parskip}{0.5em}
%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{mathtools}   % loads »amsmath«
\usepackage{graphicx}
\usepackage{caption}
\usepackage{physics}
\usepackage{subcaption}
\usepackage{tikz}

\usepackage{hyperref}
\hypersetup{
colorlinks=true,
linkcolor=magenta
}

\def \hb{\hat{\beta}}
\def \hs{\hat{s}}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \he{\hat{\varepsilon}}
\def \v1{\vec{1}}
\def \e{\varepsilon}
\def \z{z}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление

%% Шрифты
\usepackage{euscript}	 % Шрифт Евклид
\usepackage{mathrsfs} % Красивый матшрифт

\title{Сюжеты с кружка по эконометрике}
\author{Последнее обновление:}

\usepackage{geometry}
\geometry{
	a4paper,
	left=20mm,
	top=20mm,
	right=20mm
}
\setlength{\parindent}{0cm}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\Var}{Var}
\begin{document}
	
\maketitle

\tableofcontents

\section{Пределы по вероятности}
\subsection{Разминка}
Найдите:
\begin{enumerate}
	\item $\text{plim} \bar{X}_n$
	
	\item $\text{plim} \frac{\sum_{i = 1}^{n}(X_i - \bar{X})^2}{n - 1}$
	
	\item Считая, что пары $(x_1, y_1), (x_2, y_2) \dots (x_n, y_n)$ независимы и одинаково распеределены, найдите:
	
	$ \text{plim} \frac{\sum_{i = 1}^n(x_i - \bar{x_i})(y_i - \bar{y})}{n - 1}$
	
	
\end{enumerate}





\subsection{Задача про русскую народную скромность}

Отчаянный исследователь Эйзенхорн пытается предсказать результаты студентов по эконометрике и строит следующую регрессию:
\[
y_i = \beta_1 + \beta_2x_i + u_i
\]


$y_i$ - результат по эконометрике\\
$x_i$ - количество съеденных бургеров

Для получения данных Эйзенхорн заставил студентов заполнить табличку. Получилось, что регрессор подчиняется следующему утверждению:

\[
x_i^* = x_i + \alpha + \nu_i
\]

$\alpha$ - Параметр русской народной скромности\\
$\nu_i$ - случайная величина, отражающая несовершенство памяти

$E(x_i) = \mu_x, \E(u_i) = 0, \E(\nu_i) = 0, Var(x_i) = \sigma_x^2, Var(u_i) = \sigma_u^2, Var(\nu_i) = \sigma_\nu^2$\\

Найдите:\\
$\text{plim} \hat{\beta}_2$\\
$\text{plim} \hat{\beta}_1$

\subsection{Смещённость оценок}

Дети Империума Аида и Саша каждый день покупают айфоны. 

$x_i$ - количество купленных айфонов

$y_i$ - потраченная сумма денег

\[
y_i = \beta_1 + \beta_2 x_i + u_i
\]

Они оба записывают в списки количество купленных айфонов.

Аида записывает: $x_i^a = x_i + \nu_i$

Саша записывает: $x_i^s = x_i + \phi_i$

Четвёрки $(x_i, u_i, \nu_i, \phi_i)$ независимы. Величины внутри четвёрок также независимы.


$E(x_i) = \mu_x, \E(u_i) = 0, \E(\nu_i) = 0, E(\phi_i) = 0, Var(x_i) = \sigma_x^2, Var(u_i) = \sigma_u^2, Var(\nu_i) = \sigma_\nu^2, Var(\phi_i) = \sigma_\phi^2$

\begin{enumerate}
	\item $\hat{y}_i = \hb_1^a + \hb_2^ax_i^a$
	
	Найдите $plim \hb_2^a$
	
	\item $\hat{y}_i = \hb_1^s + \hb_2^sx_i^s$
	
	Найдите $plim \hb_2^s$
	
	\item Придумайте оценку $\hb_2$, у которой $plim \hb_2 = \beta_2$
	
\end{enumerate}

\subsection{}
$y_i = \beta_1 + \beta_2 x_i + u_i$
$\E(u_i) = 0$, $Var(u_i)=\sigma^2$, $x_i = i$

Найдите $plim \hb_2$

\subsection{}
$y_i = \beta_1 + \beta_2 x_i + u_i$
$\E(u_i) = 0$, $Var(u_i)=\sigma^2$, 
$x_i = 
\begin{cases}
0, \text{ если } i = 1; \\
1, \text{ если } i > 1; \\
\end{cases}
$

Найдите $plim \hb_2$

\subsection{}
$y_i = \beta_1 + \beta_2 x_i + u_i$
$\E(u_i) = 0$, $Var(u_i)=2^i\sigma^2$, 
Чётные $x_i$ равны нулю, нечётные $x_i$ равны единице.

Найдите $plim \hb_2$

\subsection{Гетерогенный эффект воздействия и ошибка специффикации}

Рассмотрим модель $y_{i} = \beta_{i} \cdot x_{i} + \alpha_{i} \cdot w_{i} + \epsilon_{i}$.

Здесь $y_{i}$ -- производительнось труда $i$-го работника; $x_{i}$ -- стаж труда $i$-го работника, $E(x_i)>0$, $0<Var(x_i)<\infty$; $w_{i}$ - бинарная переменная, равная единице, если $i$-ый работник посетил курсы повышения кваллификации, и равная нулю в противном случае; $\alpha_{i}$ - изменение производительности труда $i$-ого работника в результате посещения курсов повышения кваллификации. Обратите внимание, что эффект различен для разных работников, то есть является гетерогенным. $E(\epsilon_{i}|x_{i}, \alpha_{i}, w_{i})=0$, a векторы $v_{1}, \dots, v_{n}$ - независимы и одинаково распределены, где $v_{i}' = (x_{i}, w_{i}, \alpha_{i}, e_{i})$.

Исследователя интересует {\it средний} эффект воздействия курсов повышения кваллификации $\alpha:\ \alpha = E(\alpha_{i})$. В качестве его оценки он использует МНК-оценку коэффициента при переменной $w$ в регрессии: $\hat y = \hat \beta_{i} \cdot x_{i} + \hat \alpha_{i} \cdot w_{i}$.

Обратите внимание, что исследователь игнорирует гетерогенность, мотивируя тем, что его интересует только {\it средний} эффект воздействия.

Дополнительно известно, что в действительности эффект от посещения курсов повышения кваллификации зависит от опыта работника: $\alpha_{i} = \gamma \cdot x_i$, где $\gamma>0$. 

\begin{enumerate}
	\item  Будет ли оценка, полученная исследователем, состоятельной? Если нет, то можете ли вы определить направление ее асимптотического смещения?
	\item В предыдущем пункте на кружке получилось доказать только что при $w_{i} = 1: \plim_{n \to \infty} \hat \alpha = 0 < \gamma \cdot E(x_{1})$. Выясните, можно ли подобрать $w_{i}$ так, что $\plim_{n \to \infty} \hat \alpha > \gamma \cdot E(x_{1})$.
	\item Пусть теперь известно, что $x_{i}$ и $w_{i}$ независимы. Ответьте на вопросы предыдущего пункта.
\end{enumerate}

\subsection{}

Привести пример, когда одновременно выполняется:
\[
\lim_{n \to \infty}
E(\hat \theta_{n} - \theta) = 0
\]
\[
\plim_{n \to \infty}
(\hat \theta_{n} - \theta) \neq 0
\]

Также приведите пример, когда выполняется обратное соотношение.


\subsection{Решения}

\subsubsection*{Разминка}

\begin{enumerate}
	\item $\text{plim} \bar{X}_n = E(X_i)$
	
	\item $\text{plim} \frac{\sum_{i = 1}^{n}(X_i - \bar{X})^2}{n - 1} = \text{plim} \frac{\sum_{i = 1}^nX_i^2 - n \bar{X}^2}{n - 1} \cdot \frac{n}{n} = \left( \text{plim} \frac{\sum_{i = 1}^n X_i^2 }{n} - \text{plim} \bar{X}^2 \right) \cdot \text{plim}  \frac{n}{n-1} = \E(X_1^2) - \E(X_1)^2 = Var(X_1)$
	
	\item $ \text{plim} \frac{\sum_{i = 1}^n(x_i - \bar{x})(y_i - \bar{y})}{n - 1} = \text{plim}\frac{\sum_{i = 1}^n(x_i - \bar{x}y_i)}{n - 1} = \text{plim} \frac{\sum_{i = 1}^n(x_iy_i - \bar{x}y_i)}{n - 1} = \left( \text{plim} \frac{\sum_{i = 1}^nx_iy_i}{n} - \E(x_i)\E(y_i) \right) \cdot \text{plim} \left( \frac{n}{n-1} \right) = \E(XY) - \E(X)\E(Y) = (X, Y)$
	
	
\end{enumerate}
\subsubsection*{Задача про русскую народную скромность}
$\text{plim} \hat{\beta}_2 = \frac{\text{plim} \left( \frac{\sum_{i = 1}^n(x_i^* - \bar{x}^*)(y_i - \bar{y})}{n-1} \right)}{\text{plim} \left( \frac{\sum_{i = 1}^n(x_i^* - \bar{x}^*)^2}{n-1}\right)} = \frac{Cov(x_1^*, y_1)}{\Var(x_1^*)} = \frac{{Cov}(x_1 + \alpha + \nu_1, \beta_1 + \beta_2x_1 + \epsilon_1)}{\Var(x_1^*)} = \frac{\beta_2\Var(x_1)}{\Var(x_1^*)} = \frac{\beta_2\Var(x_1)}{\Var(x_1 + \alpha + \nu_1)} = \frac{\beta_2\Var(x_1)}{\Var(x_1) + \Var(\nu_1)} = \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\nu^2}$\\

Выводы:

\begin{enumerate}
	\item Оценки занижаются, $\text{plim} \hat{\beta_2} \neq \beta_2$
	\item $\alpha$  не влияет на смещение
	\item При $\sigma_\nu^2 = 0$ (все студенты идеально помнят свои $x_i$, и просто занижают из-за русской национальной скромности) $\Rightarrow$ $\text{plim}\hat{\beta_2} = \beta_2$
\end{enumerate}

$\plim \hat{\beta}_1 = \plim (\bar{y} - \hat{\beta}_2\bar{x}) = \plim \bar{y} - \plim \hat{\beta}_2 \cdot \plim \bar{x} = \plim \left( \frac{\sum_{i = 1}^n \beta_1 + \beta_2 x_i + \epsilon_i}{n} \right) - \beta_2 \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\nu^2} \mu_x = \plim (\beta_1 + \beta_2\bar{x} + \bar{\epsilon}) -  \beta_2 \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\nu^2} \mu_x  = \beta_1 + \beta_2 \mu_x -  \beta_2 \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\nu^2} \mu_x = \beta_1 + \frac{\beta_2 \mu_x (\sigma_x^2 + \sigma_v^2) - \beta \sigma_x^2 \mu_x}{\sigma_x^2 + \sigma_\nu^2} = \beta_1 + \frac{\beta_2 \mu_x \sigma_\nu^2}{\sigma_x^2 + \sigma_\nu^2}$

\subsubsection*{Смещённость оценок}

\begin{enumerate}
	\item $\plim \hb_2^a = \beta_2 \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\nu^2}$ где $\sigma_x^2 + \sigma_\nu^2 = \Var(x_1^a)$
	
	\item $\plim \hb_2^s = \beta_2 \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\phi^2}$ где $\sigma_x^2 + \sigma_\phi^2 = \Var(x_1^s)$
	
	\item $\plim \frac{\sum_{i = 1}^n(x_i^s - \bar{x}^s)(x_i^a - \bar{x}^a)}{n - 1} = \sigma_x^2$ 
	
	$\plim \frac{\sum_{i = 1}^n(x_i^a - \bar{x}^a)^2}{n - 1}  = \Var(x_1^a) = \sigma_x^2 + \sigma_\nu^2$
	
	$\plim \hb_2^a = \plim \frac{\sum_{i = 1}^n(x_i^a - \bar{x}^a)(y_i - \bar{x}^a)}{\sum_{i = 1}^n(x_i^a - \bar{x}^a)^2} = \beta_2 \frac{\beta_2\sigma_x^2}{\sigma_x^2 + \sigma_\nu^2}$
	
	$\hb_2 = \hb_2^a \frac{\sum_{i = 1}^n(x_i^a - \bar{x}^a)^2 / (n - 1)}{\sum_{i = 1}^n(x_i^a - \bar{x}^a)(x_i^s - \bar{x}^s) / (n - 1)} = \frac{\sum_{i = 1}^n(x_i^a - \bar{x}^a)(y_i - \bar{y})}{\sum_{i = 1}^n(x_i^a - \bar{x}^a)(x_I^s - \bar{x}^s)} = \beta_2^*$
	
	$\plim \beta_2^* = \beta_2$
	
	Другая оценка:
	
	$\beta_2^{**} = \frac{\sum_{i = 1}^n(x_i^s - \bar{x}^s)(y_i - \bar{y})}{\sum_{i = 1}^n(x_i^a - \bar{x}^a)(x_I^s - \bar{x}^s)}$
	
	$\plim \beta_2^{**} = \beta_2$
	
	Ещё одна оценка:
	
	$\beta_2^{***} = \beta_2^{**} \frac{n}{n-1}$
	
\end{enumerate}

\subsubsection*{1.4}

$\plim \frac{\sum_{i = 1}^n(x_i - \bar{x})y_i}{\sum_{i = 1}^n(x_i - \bar{x})^2}$

Подставим в это выражение $y_i = \beta_1 + \beta_2x_i + u_i$. Получим сумму в из трёх слагаемых. Рассмотрим каждое в отдельности.

\begin{enumerate}
	\item $\frac{\sum_{i = 1}^n(x_i - \bar{x})\beta_1}{\sum_{i = 1}^n(x_i - \bar{x})^2} = \beta_1 \frac{\sum_{i = 1}^n(x_i - \bar{x})}{\sum_{i = 1}^n(x_i - \bar{x})^2} = \frac{0}{\sum_{i = 1}^n(x_i - \bar{x})^2} = 0$
	
	\item $\frac{\sum_{i = 1}^n(x_i - \bar{x})\beta_2x_i}{\sum_{i = 1}^n(x_i - \bar{x})^2} = \beta_2 \frac{\sum_{i = 1}^n x_i^2 - x_i\bar{x}}{\sum_{i = 1}^n(x_i - \bar{x})^2} = \beta_2$
	
	\item Подсчёт третьей части затруднён так как есть случайная составляющая. Законом Больших чисел мы также не можему воспользоваться. Однако есть другой способ. 
	
	Пусть $\E(R_n) \to \mu, \Var(R_n) \to 0$
	
	Тогда согласно неравенству Чебышёва $P(|R_n - \mu| > \epsilon) \to 0$. То есть, если мы докажем что дисперсия этого слагаемого стремится к нулю, то предел по вероятности будет равен математическому ожиданию.
	
	Обозначим третье слагаемое как $C_n$
	
	$C_n = \frac{\sum_{i = 1}^n(x_i - \bar{x})u_i}{\sum_{i = 1}^n(x_i - \bar{x})^2}$
	
	$\E(C_n) = 0$
	
	$\Var(C_n) =  \frac{\sum_{i = 1}^n(x_i - \bar{x})^2 \sigma_u^2}{\sum_{i = 1}^n((x_i - \bar{x})^2)^2} = \frac{\sigma_u^2}{\sum_{i = 1}^n(x_i - \bar{x})^2} = \frac{\sigma_u^2}{\sum_{i = 1}^n x_i^2 - n\bar{x}^2} = \frac{\sigma_u^2}{\sum_{i = 1}^n i^2 - n \left( \frac{\sum_{i = 1}^n i}{n} \right)^2}$
	
	Далее было объяснение почему эта штука в знаменателе равна нулю. Нужен график, позже отрисую.
	
	Следовательно, $\plim \hb_2 = \beta_2$
	
	
\end{enumerate}

\subsubsection*{1.5}
Аналогично считаем, что  $\plim \frac{\sum_{i = 1}^n(x_i - \bar{x})y_i}{\sum_{i = 1}^n(x_i - \bar{x})^2} = \plim A_n + \plim B_n + \plim C_n$

Помня, что $x_1 - \bar{x} = -\frac{n - 1}{n}$, и что $x_2 - \bar{x} = \frac{1}{n}$ вычислим $\sum_{i = 1}^n (x_i - \bar{x})^2$:

$ \left( -\frac{n-1}{n} \right)^2 + \sum_{i = 1}^n \frac{1}{n^2} = \left( -\frac{n-1}{n} \right)^2 + \frac{n-1}{n} = \frac{n-1}{n^2}(n - 1 + 1)  = \frac{n - 1}{n}$

Теперь можно вычислить $\plim C_n$

$\plim C_n = plim \frac{\sum_{i = 1}^n(x_i - \bar{x})u_i}{\sum_{i = 1}^n(x_i - \bar{x})^2} = 
\plim \frac{-\frac{n - 1}{n}u_1}{\frac{n-1}{n}} + \plim \frac{\frac{1}{n}\sum_{i = 1}^nu_i}{\frac{n - 1}{n}} = -\plim u_1 + 0 = -u_1$

Таким образом, $\plim \hb_2 = \beta_2 - u_1$
\section{Двухшаговый МНК}

\subsection{}

Дано: \[ y_{i} = \beta_{1} + \beta_{2} \cdot x_{i} + u_{i}\]
\[v_{i} = \begin{pmatrix}  u_{i} \\ x_{i} \end{pmatrix}; \ E(v_{i}) = \begin{pmatrix}  0 \\ \mu_{x} \end{pmatrix};
\]
\[
Var(v_{i}) = \begin{pmatrix}  \sigma_{u}^2 & c \\
c & \sigma_{x}^2 \end{pmatrix}
\]

Найдите $\plim_{n \to \infty} \hat \beta_{2}^{\text{МНК}}$


\subsection{Бытовуха}

Шаг 1: Оценим модель
\[
\hat{x}_i = \hat{\alpha}_1 + \hat{\alpha}_1 z_i
\]

Шаг 2: Оценим модель

\[
\hat{y}_i = \hb_1 + \hb_2 \hat{x}_i
\]

\[
v_{i} = \begin{pmatrix}  u_{i} \\ x_{i} \\  z_{i} \end{pmatrix}; \ E(v_{i}) = \begin{pmatrix}  0 \\ \mu_{x} \\ \mu_{z} \end{pmatrix};
\]

\[
Var(v_{i}) = \begin{pmatrix}  \sigma_{u}^2 & c & 0 \\
c & \sigma_{x}^2  & d \\
0 & d  & \sigma_z^2 \end{pmatrix}
\]

Найдите $\plim_{n \to \infty} \hat \beta_{2}, \plim_{n \to \infty} \hat \beta_{1}$

Хинт: Полученная оценка $\hat \beta_{2}$ будет в итоге оценкой метода инструментальных переменных.


\subsection{Бытовуха, версия хардкор}

Рассматривается модель: 
\[
\theta_1 + \theta_2 x_i + u_i
\]
где $x_i$ стохастический эндогенный
регрессор:

\[
x_i = \alpha_0 + \alpha_1 p_i + \alpha_2 q_i + \epsilon_i
\]


\[
v_{i} = \begin{pmatrix}  u_{i} \\ x_{i} \\  p_{i} \\ q_{i}  \\ \epsilon_{i} \end{pmatrix}; \ E(v_{i}) = \begin{pmatrix}  0 \\ \mu_{x} \\ \mu_{p} \\ \mu_{q} \\ 0\end{pmatrix};
\]

\[
Var(v_{i}) = \begin{pmatrix}  \sigma_{u}^2 & C_{xu} & 0 & 0 & 0\\
C_{xu} & \sigma_{x}^2  & C_{px} & C_{qx} & 0\\
0 & C_{px} & \sigma_p^2 & C_{pq}  & 0\\
0 & C_{qx} & C_{pq} & \sigma_q^2 & 0 \\
0 & 0 & 0 & 0 & \sigma_\epsilon^2\end{pmatrix}
\]

Шаг 1: Оценим модель
\[
\hat{x}_i = \hat{\alpha}_0 + \hat{\alpha}_1 p_i + \hat{\alpha}_2 q_i
\]

Шаг 2: Оценим модель

\[
\hat{y}_i = \hat{\theta}_1 + \hat{\theta}_2 \hat{x}_i
\]

\begin{enumerate}
	\item  Найдите $\plim_{n \to \infty} \hat \theta_{2}, \plim_{n \to \infty} \hat \theta_{1}$  и проверьте состоятельность этих оценок.
	
	\item Пусть ваша выборка состоит из 1000 наблюдений, причем вы располагаете данными о средних выборочных значениях переменных:
	
	$\bar{X} = \bar{Y} = \bar{P} = 0, \ \bar{Q} = \bar{PQ} = \bar{XQ} = \bar{P^2} = \bar{YQ} = 1, \ \bar{Q^2} = 1.5, \ \bar{XP} = \bar{YP} = 2$
	
	Вычислите состоятельную оценку параметра $\theta_2$ из предыдущего пункта.
\end{enumerate}



\section{Идеи распределений}

\subsection{Экспоненциальное распределение и распределение Пуассона через АПП}

Аксиомы Пуассоновского потока (АПП):
\begin{enumerate}
	\item Время непрерывно.
	\item На оси времени происходят точечные происшествия.	
	\begin{center}
	\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
	%uncomment if require: \path (0,137); %set diagram left start at 0, and has height of 137
	
	%Straight Lines [id:da6929201120835929] 
	\draw    (100,68) -- (387.5,67.01) ;
	\draw [shift={(389.5,67)}, rotate = 539.8] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	
	%Shape: Circle [id:dp3127741969749247] 
	\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (129,67.5) .. controls (129,65.01) and (131.01,63) .. (133.5,63) .. controls (135.99,63) and (138,65.01) .. (138,67.5) .. controls (138,69.99) and (135.99,72) .. (133.5,72) .. controls (131.01,72) and (129,69.99) .. (129,67.5) -- cycle ;
	%Shape: Circle [id:dp5786911431724271] 
	\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (218,67.5) .. controls (218,65.01) and (220.01,63) .. (222.5,63) .. controls (224.99,63) and (227,65.01) .. (227,67.5) .. controls (227,69.99) and (224.99,72) .. (222.5,72) .. controls (220.01,72) and (218,69.99) .. (218,67.5) -- cycle ;
	%Shape: Circle [id:dp5938248751179869] 
	\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (290,67.5) .. controls (290,65.01) and (292.01,63) .. (294.5,63) .. controls (296.99,63) and (299,65.01) .. (299,67.5) .. controls (299,69.99) and (296.99,72) .. (294.5,72) .. controls (292.01,72) and (290,69.99) .. (290,67.5) -- cycle ;
	
	% Text Node
	\draw (386,83) node   {$t$};
	\end{tikzpicture}
	\end{center}
	<<Точечные>> означает, что происшествие не длится, скажем, 2 секунды, а случается <<в точке>> на оси времени.
	
	\item Количества происшествий на непересекающихся интервалах независимы.
	\begin{center}
	\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
	
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
	%uncomment if require: \path (0,137); %set diagram left start at 0, and has height of 137
	
	%Straight Lines [id:da6929201120835929] 
	\draw    (14,56) -- (301.5,55.01) ;
	\draw [shift={(303.5,55)}, rotate = 539.8] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
	
	%Shape: Rectangle [id:dp30649803725484637] 
	\draw   (27,46) -- (90.5,46) -- (90.5,66) -- (27,66) -- cycle ;
	%Shape: Rectangle [id:dp7427458030588374] 
	\draw   (154.5,45) -- (264.5,45) -- (264.5,65) -- (154.5,65) -- cycle ;
	
	% Text Node
	\draw (300,71) node   {$t$};
	
	
	\end{tikzpicture}
	\end{center}
	\item Закон распределения количества происшествий стабилен во времени. Это означает, что на двух временных интервалах одинаковой длины количества происшествий распределены одинаково.
	
	\item На малом интервале времени $\Delta t$:
	\begin{itemize}
		\item вероятность двух и более происшествий мала по сравнению с $\Delta t$:
		\[
		\P(\text{2 и более происшетсвий}) = o(\Delta t).
		\]
		\item вероятность ровно одного происшествия пропорциональна $\Delta t$ с точностью до $o$-малых:
		\[
		\P(\text{ровно 1 происшествие}) = \lambda\Delta t + o(\Delta t).
		\]
	\end{itemize}

\end{enumerate}

Посмотрим, как из АПП можно вывести экспоненциальное распределение и распределение Пуассона. Рассмотрим следующий участок на оси времени:
\begin{center}
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,101.00001525878906); %set diagram left start at 0, and has height of 101.00001525878906

%Straight Lines [id:da6929201120835929] 
\draw    (14,56) -- (301.5,55.01) ;
\draw [shift={(303.5,55)}, rotate = 539.8] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Shape: Circle [id:dp4743734570710031] 
\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (68.25,55.63) .. controls (68.25,53.21) and (70.21,51.25) .. (72.63,51.25) .. controls (75.04,51.25) and (77,53.21) .. (77,55.63) .. controls (77,58.04) and (75.04,60) .. (72.63,60) .. controls (70.21,60) and (68.25,58.04) .. (68.25,55.63) -- cycle ;
%Shape: Circle [id:dp8739714732845996] 
\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (128.25,55.63) .. controls (128.25,53.21) and (130.21,51.25) .. (132.63,51.25) .. controls (135.04,51.25) and (137,53.21) .. (137,55.63) .. controls (137,58.04) and (135.04,60) .. (132.63,60) .. controls (130.21,60) and (128.25,58.04) .. (128.25,55.63) -- cycle ;
%Shape: Circle [id:dp5886124486572519] 
\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (216.25,55.63) .. controls (216.25,53.21) and (218.21,51.25) .. (220.63,51.25) .. controls (223.04,51.25) and (225,53.21) .. (225,55.63) .. controls (225,58.04) and (223.04,60) .. (220.63,60) .. controls (218.21,60) and (216.25,58.04) .. (216.25,55.63) -- cycle ;

% Text Node
\draw (300,71) node   {$t$};
% Text Node
\draw (74,71) node   {$0$};
% Text Node
\draw (134,70) node   {$a$};
% Text Node
\draw (225,70) node   {$a\ +\ \Delta a$};


\end{tikzpicture}
\end{center}

Попробуем найти вероятность того, что за промежуток времени $[0; a]$ произойдёт ровно 0 происшествий:
\[
\P(\text{0 происшествий за } [0; a]).
\]

Для этого рассмотрим вероятность того, что ровно 0 происшествий произойдёт за промежуток времени $[0; a + \Delta a]$:
\[
\P(\text{0 происшествий за } [0; a + \Delta a]).
\]
Так как $[0; a]$ и $[a; a + \Delta a]$ -- непересекающиеся интервалы, то по аксиоме 3 количество происшествий на них независимы, а значит совместная вероятность раскладывается в произведение:
\[
\P(\text{0 происшествий за } [0; a + \Delta a]) = \P(\text{0 происшествий за } [0; a]) \times \P(\text{0 происшествий за } [a; a + \Delta a]).
\]

А $\P(\text{0 происшествий за } [a; a + \Delta a])$ можно рассчитать по аксиоме 5. На этом интервале может произойти ноль, одно или два и боле событий. Так как нас интересует первый вариант, вычтем из единицы вероятности второго и третьего вариантов:
\begin{multline*}
\P(\text{0 происшествий за } [a; a + \Delta a]) = 1 - \P(\text{1 происшествие за } [a; a + \Delta a]) -\\- \P(\text{2 и более происшествий за } [a; a + \Delta a]).
\end{multline*}

А эти вероятности -- ровно то, что стоит в аксиоме 5, только в нашем случае интервал $\Delta t$ -- это $\Delta a$. Таким образом, получаем:
\begin{multline*}
\P(\text{0 происшествий за } [a; a + \Delta a]) = 1 - \P(\text{1 происшествие за } [a; a + \Delta a]) -\\- \P(\text{2 и более происшествий за } [a; a + \Delta a]) =\\= 1 - \lambda \Delta a - o(\Delta a) - o(\Delta a) = 1 - \lambda \Delta a - o(\Delta a) \text{ (по свойствам $o$-малых)}.
\end{multline*}

Вернёмся к начальному выражению:
\[
 \P(\text{0 происшествий за } [0; a + \Delta a]) = \P(\text{0 происшествий за } [0; a]) \times (1 - \lambda \Delta a - o(\Delta a)).
\]
 
Раскроем скобки:
\begin{multline*}
\P(\text{0 происшествий за } [0; a + \Delta a]) - \P(\text{0 происшествий за } [0; a]) = \P(\text{0 происшествий за } [0; a])(-\lambda \Delta a) + o(\Delta a),
\end{multline*}
так как $\P(\text{0 происшествий за } [0; a]) \times o(\Delta a) = o(\Delta a)$ (вероятность лежит в пределах от нуля до единицы, то есть мала).

Поделим обе части на $\Delta a$:
\begin{multline*}
\dfrac{\P(\text{0 происшествий за } [0; a + \Delta a]) - \P(\text{0 происшествий за } [0; a])}{\Delta a} = \P(\text{0 происшествий за } [0; a])(-\lambda) + \dfrac{o(\Delta a)}{\Delta a}.
\end{multline*}

А теперь возьмём предел правой и левой части при $\Delta a \rightarrow 0$. Заметим, что слева стоит ни что иное как производная $\P(\text{0 происшествий за } [0; a])$ (по определению производной), если мы рассматриваем эту вероятность как функцию. Также по определению:
\[
\lim\limits_{\Delta a \rightarrow 0} \dfrac{o(\Delta a)}{\Delta a} = 0.
\]

Обозначим вероятность $\P(\text{0 происшествий за } [0; a])$ как $z(a)$, то есть явно скажем, что это некоторая функция:
\[
\P(\text{0 происшествий за } [0; a]) \equiv z(a).
\]

Тогда выражение выше можно записать как:
\[
z'_{a} = -\lambda z(a) + 0.
\]

Общее решение этого дифференциального уравнения:
\[
z(a) = C e^{-\lambda a}.
\]

Заметим, что $z(a)$ уже похожа на функции распределения экспоненциального распределения и распределения Пуассона. Восстановим константу $C$. Логично предположить, что $z(0) = 1$, то есть вероятность того, что за промежуток времени $[0; 0]$ произойдёт 0 происшествий, равна единице. Отсюда:
\[
1 = Ce^0 \Rightarrow C = 1.
\]

Таким образом: 
\[
z(a) = e^{-\lambda a}.
\]

Что такое $z(a)$? По нашей записи это вероятность того, что в промежуток времени $[0; a]$ произойдёт 0 происшествий. 

Распределение Пуассона как раз моделирует случайную величину, которая показывает число событий, произошедших за фиксированный промежуток времени. Предположим, что $a = 1$, то есть $z(a) = e^{-\lambda}$ (промежуток времени фиксирован от 0 до 1). Таким образом, $z(a)$ -- это вероятность того, что случайная величина, имеющая Пуассоновское распределение, примет значение 0 на промежутке времени $[0; 1]$. Покажем это:
\[
X \sim pois(\lambda),
\]
\[
\P(X = 0) = \dfrac{e^{-\lambda} \times \lambda^0}{0!} = e^{-\lambda}.
\]

Заметим, что $z(a)$ можно интерпретировать по-другому: если это вероятность того, что за промежуток времени $[0; a]$ произойдёт 0 происшествий, то это же вероятность того, что первое происшествие произойдёт после точки $a$ на временной оси. Пусть случайная величина $Y$ показывает время до первого происшествия. Тогда:
\[
z(a) = \P(Y \ge a).
\]
Перепишем данное выражение в терминах функции распределения:
\[
P(Y \le a) = 1 - z(a) = 1 - e^{-\lambda a}.
\]
Получили функцию распределения экспоненциального распределения! Экспоненциальное распределение моделирует время между двумя происшествиями. У нас получилось, что $Y \sim exp(\lambda)$ и $Y$ -- время до первого происшествия, то есть экспоненциальное распределение также показывает время от начала отсчёта до первого происшествия. Ввиду свойства отсутствия памяти, два этих определения эквивалентны (так как после наступления нового происшествия, точка отсчёта смещается в точку этого происшествия).

Покажем, что распределение Пуассона можно вывести для любого числа происшествий. Например, проверим, что наши рассуждения верны и для $\P(\text{1 происшествие за } [0; a])$. Будем рассматривать фиксированный промежуток времени $[0; 1]$ (то есть при $a = 1$). Ожидаемый результат:
\[
X \sim pois(\lambda).
\]
\[
\P(X=1) = e^{-\lambda}\lambda.
\]

Для краткости обозначим искомую вероятность за $u(a)$:
\[
\P(\text{1 происшествие за } [0; a]) \equiv u(a).
\]

Применим ту же схему. Вероятность того, что произошло ровно 1 происшествие за промежуток времени $[0; a + \Delta a]$ раскладывается в сумму вероятностей, что ровно 1 происшествие произошло либо за $[0; a]$, либо за $[a; a + \Delta a]$. Первая вероятность -- это произведение вероятностей, что за $[0; a]$ произошло ровно одно происшествие, а за $[a + \Delta a]$ произошло ноль происшествий. Вторая вероятность -- это произведение вероятностей, что за $[0; a]$ произошло ноль происшествий, а за $[a + \Delta a]$ произошло ровно одно происшествие. Запишем эти рассуждения в наших обозначениях:
\[
u(a + \Delta a) = u(a)(1 - \lambda \Delta a - o(\Delta a)) + z(a)(\lambda \Delta a + o(\Delta a)).
\]

Снова раскроем скобки и поделим обе части на $\Delta a$:
\[
\dfrac{u(a + \Delta a) - u(a)}{\Delta a} = \lambda z(a) - \lambda u(a) + \dfrac{o(\Delta a)}{\Delta a}.
\]

В пределе при $\Delta a \rightarrow 0$ получаем:
\[
u'_a = \lambda e^{-\lambda a} - \lambda u(a).
\]
Условие $u(0) = 0$ опять же выполняется. Если решить данное дифференциальное уравнение, получим:
\[
u(a) = \dfrac{e^{-\lambda a} \lambda a}{1!}.
\]
При $a = 1$ получаем:
\[
u(a) = e^{-\lambda} \lambda.
\]

Получили, что ожидали. Можно проверить результаты и для большего числа происшествий. 

Важный факт, который получаем из двоякой интерпретации $z(a)$: если предполагаем, что время между двумя происшествиями распределено экспоненциально, то их количество распределено по Пуассону. И наоборот, если считаем, что количество происшествий распределено по Пуассону, то время между двумя происшествиями распределено экспоненциально.

\subsection{Гамма-распределение: $\gamma(k, \lambda)$}

Гамма распределение будем рассматривать на примере ловли червячков птичкой. По смыслу, случайная величина $s \sim \gamma(k, \lambda)$ показывает суммарное время на ловлю $k$ червячков, если матожидание количества червячков за единицу времени равно $\lambda$. Раз это суммарное время, то гамма-распределение -- это ещё и сумма случайных величин, распределённых экспоненциально с параметром $\lambda$. 

Выведем общую формулу гамма-распределения по индукции, начав со случая $\gamma(3, \lambda)$. Пусть $Y_1, Y_2, Y_3 \sim exp(\lambda)$. Тогда:
\[
f(y_1, y_2, y_3)dy_1 \land dy_2 \land dy_3 = \lambda e^{-\lambda y_1}\lambda e^{-\lambda y_2}\lambda e^{-\lambda y_3}dy_1 \land dy_2 \land dy_3.
\]

Обозначим: $S_3 = Y_1 + Y_2 + Y_3$, $S_2 = Y_1 + Y_2$, $S_1 = Y_1$; $R_2 = S_1 / S_2$, $R_3 = S_2 / S_3$, $R_4 = S_3 / S_4$. По смыслу, $R_i$ -- какая доля времени потрачена на поимку $(i-1)$ червячка, если известно время на поимку $i$ червячков. Перейдём от $Y_1, Y_2, Y_3$ к $R_2, R_3, S_3$. 
\[
Y_1 = S_1 = R_2 \times R_3 \times S_3.
\]
\[
Y_2 = S_2 - S_1 = (1 - R_2) \times R_3 \times S_3.
\]
\[
Y_3 = S_3 - S_2 = (1 - R_3) \times S_3.
\]

Далее нам нужно подставить эти выражения в дифференциальную форму выше. Прежде чем сделать это, выведем часть <<с птичками>>.
\begin{align*}
	dy_1 \land dy_2 = d(r_2r_3s_3) \land d((1-r_2)r_3s_3) = (d[r_2]r_3s_3 + r_2d[r_3s_3]) \land (-d[r_2]r_3s_3 + (1-r_2)d[r_3s_3]) = \\ =
	(1-r_2)r_3s_3d[r_2] \land d[r_3s_3] + r_2r_3s_3d[r_2]\land d[r_3s_3] = r_3s_3 d[r_2] \land d[r_3s_3].
\end{align*}
\begin{align*}
	dy_1 \land dy_2 \land dy_3 &=\\= r_3s_3d[r_2] \land d[r_3s_3] \land (-d[r_3]s_3 + (1-r_3)ds_3) &=\\= r_3s_3d[r_2] \land (d[r_3]s_3 + r_3d[s_3]) \land (-d[r_3]s_3 + (1-r_3)d[s_3]) &=\\= r_3s_3d[r_2] \land (s_3(1-r_3)d[r_3]d[s_3] + s_3r_3d[r_3]d[s_3]) &=\\&= r_3s_3^2dr_2dr_3ds_3.
\end{align*}

Теперь подставим всё, что нашли, в дифференциальную форму:
\[
f(y_1, y_2, y_3)dy_1 \land dy_2 \land dy_3 = \lambda^3e^{-\lambda s_3}r_3s_3^2dr_2dr_3ds_3.
\]
Отсюда:
\[
f(r_2, r_3, s_3) = \lambda^3e^{-\lambda s_3}r_3s_3^2.
\]
Замечаем, что совместная функция плотности раскладывается на произведение индивидуальных функций плотности:
\[
f(r_2, r_3, s_3) = f(r_2)f(r_3)f(s_3) = [C_1][C_2r_3][C_3e^{-\lambda s_3}s_3^2].
\]

Восстановим константы. Понятно, что $r_2 \sim U[0, 1]$, (от 0 до 1, так как это доля времени) -- а значит, $C_1 = 1$:

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,208.00001525878906); %set diagram left start at 0, and has height of 208.00001525878906

%Shape: Axis 2D [id:dp507260345192232] 
\draw  (33,171.8) -- (239.25,171.8)(53.63,26) -- (53.63,188) (232.25,166.8) -- (239.25,171.8) -- (232.25,176.8) (48.63,33) -- (53.63,26) -- (58.63,33)  ;
%Shape: Circle [id:dp8367259082041125] 
\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (167,172.13) .. controls (167,169.85) and (168.85,168) .. (171.13,168) .. controls (173.4,168) and (175.25,169.85) .. (175.25,172.13) .. controls (175.25,174.4) and (173.4,176.25) .. (171.13,176.25) .. controls (168.85,176.25) and (167,174.4) .. (167,172.13) -- cycle ;
%Straight Lines [id:da853163758659521] 
\draw    (53.25,93) -- (171.25,93) ;


%Straight Lines [id:da046902488711099144] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (171.25,93) -- (171.13,168) ;



% Text Node
\draw (29,31) node   {$f( r_{2})$};
% Text Node
\draw (255,181) node   {$r_{2}$};
% Text Node
\draw (63,181) node   {$0$};
% Text Node
\draw (182,181) node   {$1$};
% Text Node
\draw (42,93) node   {$1$};

\end{tikzpicture}
\end{center}

Далее понимаем, что $r_3$ тоже распределено от 0 до 1, так как это доля времени. Посмотрим на функцию плотности $r_3$:

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,208.00001525878906); %set diagram left start at 0, and has height of 208.00001525878906

%Shape: Axis 2D [id:dp507260345192232] 
\draw  (33,171.8) -- (239.25,171.8)(53.63,26) -- (53.63,188) (232.25,166.8) -- (239.25,171.8) -- (232.25,176.8) (48.63,33) -- (53.63,26) -- (58.63,33)  ;
%Shape: Circle [id:dp8367259082041125] 
\draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] (167,172.13) .. controls (167,169.85) and (168.85,168) .. (171.13,168) .. controls (173.4,168) and (175.25,169.85) .. (175.25,172.13) .. controls (175.25,174.4) and (173.4,176.25) .. (171.13,176.25) .. controls (168.85,176.25) and (167,174.4) .. (167,172.13) -- cycle ;
%Straight Lines [id:da853163758659521] 
\draw    (53.63,171.8) -- (171.25,93) ;


%Straight Lines [id:da046902488711099144] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (171.25,93) -- (171.13,168) ;


%Straight Lines [id:da971385419077827] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (171.25,93) -- (51.75,94) ;



% Text Node
\draw (29,31) node   {$f( r_{3})$};
% Text Node
\draw (255,181) node   {$r_{3}$};
% Text Node
\draw (63,181) node   {$0$};
% Text Node
\draw (182,181) node   {$1$};
% Text Node
\draw (42,93) node   {$2$};

\end{tikzpicture}
\end{center}

Чтобы интеграл под функцией плотности равнялся 1, $C_2$ должна быть равна 2. Выходит, что $C_3 = \dfrac{\lambda^3}{3}$.

На самом деле, константы нам не так важны. Обобщим, что получили про гамма-распределение. Если $S_k$ -- суммарное время на ловлю $k$ червячков, если ожидаемое количество пойманных червячков за единицу времени равно $\lambda$, то $S_k \sim \gamma(k, \lambda)$ и $f(s) = Ce^{-\lambda s}s^{k-1}$.

Покажем, что это верно и для следующего шага индукции. Перейдём от $r_2$, $r_3$, $s_3$ к $r_2$, $r_3$, $r_4$, $s_4$. Тогда $s_3 = s_4 \times r_4$. Вернёмся к дифференциальной форме:
\[
1 \times 2r_3 \times \dfrac{\lambda^3}{2}e^{-\lambda s_3}s_3^2dr_2dr_3ds_3.
\]
При добавлении $y_4$ форма домножится на $\land \lambda e^{-\lambda y_4} dy_4$. Понятно, что $y_4 = (1-r_4)s_4$.

Рассчитаем новую часть <<с птичками>>.
\begin{align*}
ds_3 \land dy_4 = d(s_4r_4) \land d((1-r_4)s_4) = (d[s_4]r_4 + s_4d[r_4]) \land (-d[r_4]s_4 + (1-r_4)d[s_4]) = (r_4s_4 + s_4(1-r_4))d[r_4] \land d[s_4] =\\= s_4dr_4ds_4.
\end{align*}

Понятно, что $s_3^2 =(s_4r_4)^2$. Тогда новая дифференциальная форма имеет вид (подставляем всё, что получили):
\[
1 \times 2r_3 \times 3r_4^2 \times \dfrac{\lambda^4}{3!}s_4^3e^{-\lambda s_4}dr_2dr_3dr_4ds_4.
\] 
Совпадает с нашей формулой выше с точностью до константы. Аналогично можно сделать следующий шаг индукции для $ds_4 \land dy_5$. 

Так как гамма-распределение -- это сумма экспоненциальных, легко вывести матожидание и дисперсию гамма-распределения:
\[
\E(\gamma) = k \times \dfrac{1}{\lambda}.
\]
\[
Var(\gamma) = Var(Y_1 + \dots + Y_k) = kVar(Y_1) = k \times \dfrac{1}{\lambda^2}.
\]

Понятно, что при $k=1$ гамма-распределение -- это экспоненциальное распределение. 

Полное общее определение: $S_k \sim \gamma(k, \lambda)$, где $S_k$ -- суммарное время на ловлю $k$ червячков, $\lambda$ -- ожидаемое количество пойманных червячков за единицу времени. Тогда:
\begin{align*}
	f(s_k) &= \dfrac{\lambda^k}{(k-1)!}e^{-\lambda s_k}s_k^{k-1}. \\
	\E(S_k) &= \dfrac{k}{\lambda}. \\
	Var(S_k) &= \dfrac{k}{\lambda^2}.
\end{align*}

\subsection{Бета-распределение: $\beta(a, b)$}

По смыслу: ловим $(a+b)$ червячков, $a$ отдаём, $b$ оставляем себе. Тогда, если $R$ -- доля времени на поимку $a$ червячков, то то $R \sim \beta(a, b)$.

Пусть $Z_1$ -- время на поимку $a$ червячков, а $Z_2$ -- доля на поимку $b$ червячков. Тогда $Z_1 \sim \gamma(a, \lambda)$, $Z_2 \sim \gamma(b, \lambda)$. Дифференциальная форма:
\[
f(z_1, z_2)dz_1 \land dz_2 = \dfrac{\lambda^a}{(a - 1)!}e^{-\lambda z_1}z_1^{a-1} \times \dfrac{\lambda^b}{(b - 1)!}e^{-\lambda z_2}z_2^{b-1}dz_1 \land dz_2.
\]

Перейдём к нашим обозначениям $r$ и $s$. Понятно, что $Z_1 = Y_1 + \dots + Y_a$, $Z_2 = Y_{a+1} + \dots$ Тогда обозначим:
\begin{align*}
	s &= Z_1 + Z_2. \\
	r &= \dfrac{Z_1}{Z_1 + Z_2}.
\end{align*}
Тогда $Z_1 = rs$, $Z_2 = (1-r)s$. Часть <<с птичками>>:
\[
d(rs) \land d((1-r)s) = sdr \land ds \text{ (считали выше).}
\]
Подставляем всё в дифференциальную форму:
\[
\dfrac{\lambda^{a+b}}{(a-1)!(b-1)!}e^{-\lambda s} r^{a-1} (1-r)^{b-1} s^{(a-1)+(b-1)+1}dr\land ds.
\]
Перегруппируем:
\[
\dfrac{\lambda^{a+b}}{(a+b-1)!}s^{a+b-1}e^{-\lambda s} \times \dfrac{(a+b-1)!}{(a-1)!(b-1)!}r^{a-1}(1-r)^{b-1}dr\land ds.
\]
Замечаем, что до знака $\times$ стоит функция плотности гамма-распределения $f(s)$ с параметрами $k=a+b$ и $\lambda$. А после этого знака -- $f(r)$, и по нашему обозначению $r$, это и есть функция плотности бета-распределения. 

Выпишем отдельно: если $R$ -- доля времени на поимку $a$ червячков, а всего ловим $(a+b)$ червячков, то $R \sim \beta(a, b)$ и:
\begin{align*}
f(r) &= \dfrac{(a+b-1)!}{(a-1)!(b-1)!}r^{a-1}(1-r)^{b-1}. \\
\E(R) &= \dfrac{a}{a+b}.
\end{align*}

\subsection{Нормальное распределение (через Гаусса)}

Предпосылки:
\begin{enumerate}
	\item Есть истинная величина $\mu$.
	\item $y_i = \mu + u_i$, $u_i$ -- независимы и симметричны около 0 (то есть свидетели равновероятно завышают и занижают показания).
	\item $\bar{y} = \hat{\mu}_{ML} \text{ } \forall y_1 \dots y_{n+1}$.
	\item[4?] $f(u)$ -- дифференцируемая.
\end{enumerate}
Тогда $u_i \sim \mathbb{N}(0, \sigma^2)$.

Выпишем правдоподобие:
\[
L = f(y_1 - \mu)f(y_2 - \mu)\dots f(y_{n+1}-\mu).
\]
\[
\ell = \sum_{i=1}^{n+1}\ln(f(y_i-\mu)).
\]
\[
\ell'_{\mu} = -\sum_{i=1}^{n+1} \dfrac{f'(y_i-\mu)}{f(y_i-\mu)}.
\]

По предпосылке 3:
\[
\sum_{i=1}^{n+1} \dfrac{f'(y_i-\bar{y})}{f(y_i-\bar{y})} = 0 \text{ } \forall y_1 \dots y_{n+1}.
\]
Так как выполняется для любых $y_i$, возьмём конкретные показания. Для них должно выполняться:
\begin{align*}
y_1 - \bar{y} &= a, \\
y_2 - \bar{y} &= a, \\
\dots \\
y_n - \bar{y} &= a, \\
y_{n+1} - \bar{y} &= -na.
\end{align*}
Получаем:
\[
n \dfrac{f'(a)}{f(a)} + \dfrac{f'(-na)}{f(-na)} = 0 \text{ } \forall n, a. 
\]
Так как $f$ -- симметричная дифференцируемая:
\[
\dfrac{f'(na)}{f(na)} = n \dfrac{f'(a)}{f(a)}.
\]
Обозначим: $h(x) = \dfrac{f'(x)}{f(x)}$. Получаем, что $h(na) = nh(a)$ $\forall n, a$. Это означает, что $h(x) = kx$. Получаем дифференциальное уравнение:
\[
\dfrac{f'(x)}{f(x)} = kx.
\]
Решение:
\[
f(x) = C_1e^{\dfrac{kx^2}{2}}.
\]

\subsection{Распределения через максимальную энтропию}

\hyperlink{entropy}{Отступление: про энтропию.}

\begin{enumerate}
	\item $\E(y) = 10$, $\sigma = 1$, $H(y) \rightarrow \max \Rightarrow y \sim N(0,1)$.
	\item $y \ge 0$, $\E(y) = 5$, $H(y) \rightarrow \max \Rightarrow y \sim Exp(0)$.
	\item $y \in [a, b]$, $H(y) \rightarrow \max \Rightarrow y \sim U[a, b]$.
\end{enumerate}

Интересный вопрос: что больше: $D_{KL}(N(0,1) || U[0, 1])$ или $D_{KL}(U[0,1] || N(0, 1)$? Посмотрим по смыслу: в перовом случае мы выбираем число из нормального распределения, а задаём вопросы про числа из равномерного распределения. Значит, существует положительная вероятность, что мы выберем такое число из нормального распределения, которое не накрывается равномерным, то есть, мы можем никогда не угадать это число. Это означает, что $D_{KL}(N || U) = +\infty$. Во втором же случае наоборот: загадали из равномерного, а спрашиваем про нормальное, то есть рано или поздно число будет угадано. Это означает, что  $D_{KL}(U || N) = $ большое, но конечное число. То есть $D_{KL}(U || N) < D_{KL}(N || U)$. 

\subsubsection{Нормальное распределение (через максимальную энтропию)}

Пусть $X \sim N(\mu, \sigma^2)$, $\E(y) = \mu$, $Var(y) = \sigma^2$. Тогда $H(x) \ge H(y)$. Покажем это.

\begin{align*}
	H(X) = \int_{-\infty}^{+\infty}\dfrac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}} \log_{\frac{1}{2}} \dfrac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}dx.
\end{align*}
Для краткости:
\[
H(X) = \int f(x) \log_{\frac{1}{2}} f(x)dx.
\]
Для удобства перейдём в наты. Напоминание:
\[
\log_{\frac{1}{2}}x \times \log_{e}\frac{1}{2} = \ln x.
\]
\[
\ln \frac{1}{2} < 0.
\]
Тогда:
\[
H(X) = -\int f(x) \ln f(x)dx.
\]
Раскроем вторые скобки:
\begin{align*}
	H(X) = -\int f(x) \left( \ln \dfrac{1}{\sqrt{2\pi\sigma^2}} - \dfrac{1}{2}\dfrac{(x-\mu)^2}{\sigma^2} \right)dx.
\end{align*}
Знаем, что:
\begin{align*}
	Var(X) = \E(X-\mu)^2 = \int f(x)(x-\mu)^2dx.
\end{align*}
А это как раз второй член в скобках. Получается, что числитель при взятии интеграла обратится в диспресию $X$ и сократится со знаменателем. Получаем:
\[
H(X) = - \left( \ln \dfrac{1}{\sqrt{2\pi\sigma^2}} - \dfrac{1}{2} \right) = \dfrac{1}{2} \left(1 + \ln(2\pi\sigma^2)\right).
\]

Пусть $Y \sim q$. Тогда:
\[
CE(Y||X) = -\int q(x) \left( \ln \dfrac{1}{\sqrt{2\pi\sigma^2}} - \dfrac{1}{2}\dfrac{(x-\mu)^2}{\sigma^2} \right)dx = \dfrac{1}{2} \left(1 + \ln(2\pi\sigma^2)\right).
\]
Получили, что:
\[
H(X) = CE(Y||X) \ge H(Y) \text{ (доказано ниже).}
\]

\subsubsection{Экспоненциальное распределение (через максимальную энтропию)}

Докажем, что если $\E(X) = 1/\lambda$ и $X \ge 0$, то $H(X)$ максимальна при $X \sim exp(\frac{1}{\lambda})$.

Аналогично нормальному распределению:
\[
H(X) = - \int_{0}^{+\infty} f(x)(\ln\lambda - \lambda x)dx = -(\ln \lambda - 1) = 1 - \ln \lambda.
\]
\[
CE(Y||X) = - \int_{0}^{+\infty} q(x)(\ln\lambda - \lambda x)dx = -(\ln \lambda - 1) = 1 - \ln \lambda.
\]
\[
H(X) = CE(Y||X) \ge H(Y).
\]

\hypertarget{entropy}{}
\section{Энтропия}
Рассмотрим игру. Пусть мы загадываем один из четырёх вариантов со следующими вероятностями:

\begin{center}
	\bgroup
	\def\arraystretch{1.5}
	\hspace*{-1.2cm}\begin{tabular}{ c| c c c c}
		$Y$ & Пуассон & Фишер & Коши & Арбуз \\
		\hline
		Prob. & 1/2 & 1/4 & 1/8 & 1/8
	\end{tabular}
	\egroup
\end{center}

Нужно найти, какое среднее число бинарных вопросов нужно задать, чтобы угадать $Y$, используя \textit{оптимальную стратегию}. 

Логично, что при данном распределении вероятностей сначала уместно задать вопрос: <<Это Пуассон?>>, потому что Пуассон загадывается наиболее вероятно. Если это Пуассон, ты игра оканчивается, если нет -- то следующим вопросом узнаём, Фишер ли это. Если же это и не Фишер, то можем спросить либо про Коши, либо про арбуз, и после этого точно завершить игру. Посмотрим, какое минимальное количество вопросов нужно задать в лучшем случае, чтобы отгадать каждый вариант, используя оптимальную стратегию:

\begin{center}
	\bgroup
	\def\arraystretch{1.5}
	\hspace*{-1.2cm}\begin{tabular}{c c c}
		$Y$ & Prob. & Q \\
		\hline
		Пуассон & 1/2 & 1 \\
		Фишер & 1/4 & 2 \\
		Коши & 1/8 & 3 \\
		Арбуз & 1/8 & 3 \\
	\end{tabular}
	\egroup
\end{center}

Заметим, что $Q = \log_{\frac{1}{2}}\text{Prob.}$ Найдём матожидание $Q$:
\[
\E(Q) = \sum \P(Y = y_i)\times \log_{\frac{1}{2}}\P(Y = y_i).
\]

Это матожидание и есть энтропия. По смыслу, энтропия показывает среднее число бинарных вопросов, которые нужно задать при использовании оптимальной стратегии, чтобы завершить игру. Обозначение: $H(y)$, $H(p)$.

Теперь рассмотрим кросс-энтропию (из одного распределения в другое). Для этого вернёмся к нашей игре и будем считать, что вероятности, которые мы рассматривали до этого, -- это истинное распределение вероятностей $A$. Также считаем, что это истинное распределение нам неизвестно, и свою стратегию отгадывания мы будем строить по другому распределению, $B$:

\begin{center}
	\bgroup
	\def\arraystretch{1.5}
	\hspace*{-1.2cm}\begin{tabular}{ c| c c c c}
		$Y$ & Пуассон & Фишер & Коши & Арбуз \\
		\hline
		$A$ & 1/2 & 1/4 & 1/8 & 1/8 \\
		$B$ & 1/4 & 1/2 & 1/8 & 1/8
	\end{tabular}
	\egroup
\end{center}

Так как мы строим стратегию по распределению $B$, то сначала уместно задать вопрос по Фишера и только затем про Пуассона. Соответственно, минимальное количество задаваемых вопросов так же изменится:

\begin{center}
	\bgroup
	\def\arraystretch{1.5}
	\hspace*{-1.2cm}\begin{tabular}{c c c}
		$Y$ & Prob. (по $A$) & Q (по $B$) \\
		\hline
		Фишер & 1/4 & 1 \\
		Пуассон & 1/2 & 2 \\
		Коши & 1/8 & 3 \\
		Арбуз & 1/8 & 3 \\
	\end{tabular}
	\egroup
\end{center}

Таким образом, изменится и матожидание $Q$, то есть энтропия. Теперь это матожидание называется кросс-энтропией из $A$ в $B$. По смыслу: кросс-энтропия показывает среднее число бинарных вопросов, которое, как мы считаем в соответствии с распределением $B$, нужно задать при использовании оптимальной стратегии, чтобы завершить игру, истинное распределение вероятностей которой -- $A$. Обозначение: $CE(a || b)$, $Ha(b)$. Понятно, что $H(a) = CE(a || a)$. То есть:
\[
CE(A||B) = \sum \P_A(Y = y_i)\times \log_{\frac{1}{2}}\P_B(Y = y_i).
\]

Рассчитаем энтропию и кросс-энтропию для нашей игры.
\[
H(A) = \dfrac{1}{2} \times 1 + \dfrac{1}{3} \times 2 + 2 \times 3 \times \dfrac{1}{8} = 1\dfrac{3}{4}.
\]
\[
CE(A||B) = \dfrac{1}{4} \times 1 + \dfrac{1}{2} \times 2 + 2 \times 3 \times \dfrac{1}{8} = 2.
\]

Получили, что $CE(a||b) > H(a)$. Совпадение ли это? Прежде чем выяснить это, введём следующую величину.

\textbf{Дивергенция Куйбака-Лейблера} показывает, насколько в среднем больше вопросов потребуется задать, чем могли бы при использовании оптимальной стратегии. По смыслу, это некоторая мера потери. Обозначение:
\[
D_{KL}(a||b) = CE(a||b) - H(a).
\]

\textbf{Теорема:}
\[
CE(a||b) \ge CE(a||a),
\]
\[
D_{KL}(a||b) \ge 0.
\]
\textbf{Доказательство:}
Построив графики $y=x-1$ и $y=\ln x$, можно убедиться, что $x-1 \ge \ln x$. Подставим $x = \dfrac{a_i}{b_i}$. Получаем:
\begin{align*}
	\dfrac{a_i}{b_i} - 1 &e\ge \ln\dfrac{a_i}{b_i} \\
	\sum(a_i - b_i)&\ge \sum(b_i\ln a_i - b_i \ln b_i).
\end{align*}

Так как $A$ и $B$ -- распределения вероятностей, слева стоит разность $1 - 1 = 0$. Получаем:
\[
\sum(b_i\ln a_i) - \sum(b_i \ln b_i) \le 0.
\]
Перейдём от натуральных логарифмов к логарифмам с основанем $\dfrac{1}{2}$. Для этого домножим обе части неравенства на $\log_{\frac{1}{2}}e$, потому что:
\[
\log_e a \times \log_{\frac{1}{2}}e = \log_{\frac{1}{2}}e^{log_e a} = \log_{\frac{1}{2}}a.
\]
Тогда получим:
\[
\sum(b_i  \log_{\frac{1}{2}} a_i) - \sum(b_i  \log_{\frac{1}{2}} b_i) \ge 0,
\]
так как $ \log_{\frac{1}{2}}e < 0$. Таким образом,
\[
\sum(b_i  \log_{\frac{1}{2}} a_i) \ge \sum(b_i  \log_{\frac{1}{2}} b_i)
\]
\[
CE(b||a) \ge CE(b||b).
\]

В непрерывном случае всё сохраняется, но все суммы заменяются на интегралы. Энтропия в непрерывном случае плохо интерпретируется. Например:
\begin{align*}
D_{KL}(a||b) = \int_{-\infty}^{+\infty}a\log(b) dt - \int_{-\infty}^{+\infty}a\log(a) dt = \int_{-\infty}^{+\infty}a\log (\dfrac{b}{a}) dt.
\end{align*}

В дискретном случае, скорее, хотим минимизировать энтропию, а в непрерывном -- $D_{KL}$.

Пример из Variational Bayes. Имеется апостериорное распределение $P(\theta|y)$, хитрое и странное. Возьмём другое распределение, $q(\theta)$, так, чтобы оно было простым и было похоже на апостериорное. Предположим, что:
\[
q(\theta) = q_1(\theta_1) \times q_2(\theta_2).
\] 
\[
\theta_1 \sim N(\mu_1, \sigma_1^2).
\]
\[
\theta_2 \sim N(\mu_2, \sigma_2^2).
\]

Что проще минимизировать по $q$ (ответ в разделе про VB):
\begin{enumerate}
	\item $D_{KL}(q||p) = \int q \ln \dfrac{p}{q}dt$.
	\item $D_{KL}(p||q) = \int p \ln \dfrac{q}{p}dt$.
\end{enumerate}


\section{Тета-метод}

Тета-метод -- частный случай ETS(AAN), поэтому он не подходит для анализа сезонных рядов. Напоминание: ETS(AAN):
\[
\varepsilon \sim N(0, \sigma^2)
\]
\[
\begin{cases}
	y_t = l_{t-1} + b_{t-1} + \varepsilon_t, \\
	l_t =  l_{t-1} + b_{t-1} + \alpha\varepsilon_t, \\
	b_t = b_{t-1} + \beta\varepsilon_t.
\end{cases}
\]
ML: $\alpha$, $\beta$, $\sigma^2$, $l_0$, $b_0$.

Тета-метод: $\beta = 0$, $l_1 = y_1$ (положим линию долгосрочного уровня так, чтобы было попадание на первое наблюдение). Следовательно:
\begin{align*}
	y_t &= l_{t-1} + b + \varepsilon_t. \\
	l_t &= l_{t-1} + b + \alpha\varepsilon_t. \\
	b_t &= b_0 = b. \\
	y_1 &= b_1 = b.
\end{align*}
Тогда:
\begin{align*}
\Delta y_t &= \Delta l_{t-1} + \Delta b + \Delta \varepsilon_t. \\
\Delta y_t &= b + \alpha \varepsilon_{t-1} + \varepsilon_t - \varepsilon_{t-1}. \\
\Delta y_t &= b + \varepsilon_t + (\alpha - 1)\varepsilon_{t-1}.
\end{align*}
А это ARIMA(0, 1, 1).

Тета-метод получается лучше, чем ETS-AAN.

\section{Байесовский подход}

Задача:
\begin{enumerate}
	\item Данные: $y_1 = 3$, $y_2 = 5$.
	\item Модель: $y_i \sim N(\mu, \dfrac{1}{\tau})$ и независимы. 
	\item Априорное мнение о $\mu$ и $\tau$: $\tau \sim \gamma(2, 7)$, $\mu | \tau \sim N(1, \dfrac{2}{\tau})$.
\end{enumerate}

Нужно найти апостериорное распределение $f(\mu, \tau | y)$:
\begin{enumerate}
	\item[a)] Явно: N-IG (нормальное обратное гамма, нормальное обратное Уишарта): $\sigma^2 \sim IG$, $\mu | \sigma^2 \sim N$.
	\item[b)] Описать явно алгоритм MH-RW.
	\item[c)] Описать VB (вариационный Байес).
\end{enumerate}

\subsection{Явно (N-IG)}

Априорное распределение:
\[
f(\tau) = Ce^{-7\tau}\tau^{2-1} \propto e^{-7\tau}\tau.
\]
\[
f(\mu | \tau) = \dfrac{1}{\sqrt{2\pi\frac{2}{\tau}}}e^{-\frac{1}{2}(\mu - 1)^2\frac{\tau}{2}}.
\]

Модель:
\[
f(y_i | \mu, \tau) = \dfrac{1}{\sqrt{2\pi \frac{1}{\tau}}}e^{-\frac{1}{2}(y_i - \mu)^2\tau}.
\]

Тогда выводим апостериорное распределение:
\begin{align*}
	f(\mu, \tau | y_1, y_2) = \dfrac{f(\mu, \tau, y_1, y_2)}{f(y_1, y_2)} \propto f(\mu, \tau, y_1, y_2) = f(\mu, \tau)f(y_1, y_2|\mu, \tau) =\\= [\text{априорное распределение}\times\text{функция правдоподобия}] = f(\tau)f(\mu|\tau)f(y_1|\mu, \tau)f(y_2|\mu, \tau) \propto \\ \propto
	\tau e^{-7\tau}\sqrt{\tau}e^{-\frac{1}{2}(\mu - 1)^2\frac{\tau}{2}}\sqrt{\tau}e^{-\frac{1}{2}(3 - \mu)^2\tau}\sqrt{\tau}e^{-\frac{1}{2}(5 - \mu)^2\tau} \propto \tau^{2.5}e^{-7\tau} e^{-\frac{1}{2}\tau (\frac{(\mu-1)^2}{2})+(\mu-3)^2+(\mu-5)^2} = \\ =
	\tau^{2.5}e^{-7\tau}e^{-\frac{1}{2}\tau(2.5\mu^2 - 17\mu + 34.5)} = \tau^{2.5}e^{-7\tau}e^{-\frac{1}{2}2.5\tau((\mu - \frac{17}{5})^2 + (\frac{69}{5}-(\frac{17}{5})^2))} \propto \underbrace{\tau^{2.5}e^{-7\tau-\frac{\tau}{4}(69 - \frac{17^2}{5})}}_{\gamma(2.5-0.5+1;\text{ } 7 + \frac{69 - \frac{17^2}{5}}{4})} \times \underbrace{\underbrace{C}_{\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{\frac{1}{2.5\tau}}}}e^{-\frac{1}{2}2.5\tau(\mu - \frac{17}{5})^2}}_{N(\frac{17}{5}; \frac{1}{2.5\tau})}.
\end{align*}

В итоге, снова получили NIG. Было: 
\begin{align*}
	\tau &\sim \gamma(a, b). \\
	\mu | \tau &\sim N(\mu_0, \frac{k}{\tau}), \\
	y_1 &\dots y_n.
\end{align*}
Стало:
\begin{align*}
	\tau | y_1, y_2 &\sim \gamma(a^*, b^*), \\
	\mu | \tau, y_1, y_2 &\sim N(\mu_0^*, \dfrac{k^*}{\tau}), \\
\end{align*}

Параметры априорного распределения можно восстановить в общем виде, например:
\[
a^* = a + \dfrac{n}{2},
\]
потому что к изначальному $a-1$ в степени $\tau$ придёт $n\times 0.5$ из функций плотности $y_i$ и $0.5$ из функции плотности $\mu$, а в конце нужно вычесть одну $0.5$ и прибавить $1$ для восстановления константы нормального распределения. Получаем, что $a^* = a - 1 + 0.5n + 0.5 - 0.5 + 1$.

\subsection{MH-RW}

Цель -- получить набор $\left( \mu^{[1]},\tau^{[1]}  \right)$, $\left( \mu^{[2]},\tau^{[2]}  \right)$, $\left( \mu^{[3]},\tau^{[3]}  \right)$, $\ldots$ Вопрос -- как это сделать.

\begin{description}
	\item[Шаг 1:] Генерируем случайно $\tau^{[1]} \sim f(\tau)$. Получаем $\mu^{[1]}$. В нашем случае: 
	\begin{align*}
	\tau^{[1]} &\sim \gamma(2, 7), \\
	\mu^{[1]} &\sim N(1, \frac{2}{\tau^{[1]}}).
	\end{align*}
	
	\item[Шаг 2:] Сочиняем предложение:
	\begin{align*}
		\tau^{[\text{prop}]} &= \tau^{[1]} + N(0; 4), \\
		\mu^{[\text{prop}]} &= \mu^{[1]} + N(0; 1).
	\end{align*}
	Прибавляемые распределения -- вопрос настройки алгоритма.
	
	\item[Шаг 3:] С некоторой вероятностью предложение одобряется, и тогда $\tau^{[2]} = \tau^{[\text{prop}]}$, $\mu^{[2]} = \mu^{[\text{prop}]}$, а затем переходим к шагу 2. Если предложение не одобряется, то просто переходим к шагу 2.
	
\end{description}
Как получить вероятность перехода?

\textbf{Основная идея:} если $\theta^{[n]}$ ($\theta = (\mu, \tau)$) уже генерируется из $f(\theta | y_1, y_2)$, то \textbf{не портить!}

Достаточное условие:
\[
s(\theta_A \rightarrow \theta_B) = s(\theta_B \rightarrow \theta_A),
\]
то есть количество переходов из $A$ в $B$ за единицу времени равно количеству переходов из $B$ в $A$ за единицу времени.

Распишем это условие в плотностях:
\begin{align*}
	f(\theta_A|y) \times f_{N(0,1)}(\mu_B - \mu_A)\times f_{N(0,4)}(\tau_B - \tau_A)\times\alpha(\theta_A \rightarrow \theta_B) =\\= f(\theta_B|y) \times f_{N(0,1)}(\mu_A - \mu_B)\times f_{N(0,4)}(\tau_A - \tau_B)\times\alpha(\theta_B \rightarrow \theta_A),
\end{align*}

где $\alpha(\cdot)$ -- вероятность одобрения перехода, указанного в скобках. Функции плотности нормального распределения сокращаются, и получаем следующее условие:
\[
\dfrac{\alpha(\theta_B \rightarrow \theta_A)}{\alpha(\theta_A \rightarrow \theta_B)} = \dfrac{f(\theta_A|y)}{f(\theta_B|y)}.
\]

А $f(\theta_A|y)$ и $f(\theta_B|y)$ знаем из пункта а): 
\begin{align*}
f(\theta_A|y) = f(\mu_A, \theta_A|y_1, y_2), \\
f(\theta_B|y) = f(\mu_B, \theta_B|y_1, y_2).
\end{align*}

Пример: $f(\theta_A|y) = 0.0012$, $f(\theta_B|y) = 0.0036$. Тогда, например, $\alpha(\theta_A \rightarrow \theta_B) = 1$, $\alpha(\theta_B \rightarrow \theta_A) = \dfrac{1}{3}$. Для вероятностей одобрения можно взять и другие числа (например, $\dfrac{1}{5}$ и $\dfrac{1}{15}$), но не берут, так как сходимость дольше (3 предложения на один шаг против 15 предложений на один шаг).

В компьютере хранятся не сами функции плотности, а их логарифмы, чтобы не было выхода за размеры хранения чисел. Тогда:
\[
\ln\alpha(\theta_B \rightarrow \theta_A) - \ln\alpha(\theta_A \rightarrow \theta_B) = \ln f(\theta_A|y) - \ln f(\theta_B|y).
\]

\subsection{Вариационный Байес}

Явный алгоритм -- точный, на конечной выборке -- примерный. MH-RW -- асимптотически точный. VB -- примерный. То есть через VB получаем распределение, похожее на апостериорное, но сколько ни жди, точным он не будет.

Идея:
\begin{align*}
	&f(\mu, \tau | y), \\
	\tau | y &\sim \gamma(\ldots), \\
	\mu | \tau, y &\sim N(\ldots),
\end{align*}
$\mu$ и $\tau$ зависимы.

Чаще всего: $q(\mu, \tau) = q(\mu)q(\tau)$.
Приближаем распределениями: $\mu \sim N(?; ?)$, $\tau \sim N(?; ?)$, параметры которых надо подобрать.
\begin{align*}
	D_{KL}(q||f(\theta|y)) \ge 0, \\
	D_{KL}(f(\theta|y)||q) \ge 0. 
\end{align*}

Что проще минимизировать по $q$:
\begin{enumerate}
	\item $D_{KL}(q||f(\theta|y)) = \int_{\theta} \underbrace{q(\theta)}_{\text{простая}} \ln \dfrac{f(\theta|y)}{q(\theta)}d\theta$.
	\item $D_{KL}(f(\theta|y)||q) = \int_{\theta} \underbrace{f(\theta|y)}_{\text{адская}} \ln \dfrac{q(\theta)}{f(\theta|y)}d\theta$.
\end{enumerate}

Вывод: по $q$ легче минимизировать первую функцию. Упростим далее:
\begin{align*}
	\int_{\theta} q(\theta) \ln \dfrac{f(\theta|y)}{q(\theta)}d\theta = \int_{\theta} q(\theta) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta - \int_{\theta} q(\theta)\ln f(y)d\theta = \int_{\theta} q(\theta) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta - \ln f(y).
\end{align*}

Получается, что минимизировать нужно только уменьшаемое:
\[
\int_{\theta} q(\theta) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta = \int_{\theta} q(\mu)q(\tau) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta \rightarrow \min_{q}.
\]

Процедура VB с высоты птичьего полёта:
\begin{description}
	\item[Шаг 1:] Подбираем $q_1(\mu)$, чтобы $\min \int_{\theta} q(\theta) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta$.
	
	\item[Шаг 2:] Подбираем $q_2(\tau)$, чтобы $\min \int_{\theta} q(\theta) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta$. Далее переходим к шагу 1.
\end{description}

Как подбирать? Сделаем вспомогательное упражнение. В $\int_{\theta} q(\theta) \ln \dfrac{f(\theta,y)}{q(\theta)}d\theta$ выделить часть, которая зависит от $q_1$. 
\begin{align*}
	\iint q_1(\mu)q_2(\tau) \ln \dfrac{f(\mu, \tau, y)}{q_1(\mu)q_2(\tau)}d\mu d\tau =\\= \iint q_1(\mu) q_2(\tau) \ln f(\mu, \tau, y)d\mu d\tau - \iint q_1(\mu)q_2(\tau)\ln q_1(\mu) d\mu d\tau - \iint q_1(\mu)q_2(\tau)\ln q_2(\tau) d\mu d\tau.
\end{align*}

Помня, что минимизируем по $q_1$, видим, что последний член -- это константа, так как $q_1$ интегрируется в 1, а остальное не зависит от $q_1$. В двух первых членах вынесем $q_2(\tau)$ за знак внутреннего интеграла и скомбинируем. Получаем:
\[
\int q_2(\tau) \int q_1(\mu)(\ln f(\mu, \tau, y) - \ln q_1(\mu))d\mu d\tau.
\]

Как это минимизировать по $q$? \hyperlink{euler}{Отступление: уравнение Эйлера.}

<Продолжение следует>

\hypertarget{euler}{}
\section{Уравнение Эйлера}
\[
\int_{0}^{1} g(y, \dot{y}, t)dt \rightarrow \max_y.
\]

Пример:
\[
\int_{0}^{1} (\dot{y} - 7)^2 dt \rightarrow \min_y.
\]

Понятно, что $y(t) = 7t + k$.

Рассмотрим возмущения нашей функции:
\[
y(t) + \delta v(t) = \tilde{y}(t),
\]
где $\delta$ -- число, а $v(t)$ -- возмущение. Тогда:
\[
\dot{\tilde{y}}(t) = \dot{y}(t) + \delta \dot{v}(t).
\]
Идея: если нашли экстремум, модифицировать функцию каким угодно возмущением не выгодно, а значит, можно минимизировать по $\delta$, то есть:
\[
\dfrac{d\left[ \int_{0}^{1} f(y + \delta v; \dot{y} + \delta \dot{v}; t)dt \right]}{d\delta} = 0, \text{ } \forall v.
\]

Пусть все функции <<хорошие>>, тогда:
\begin{align*}
	\int_{0}^{1}& \dfrac{df(\ldots)}{d\delta}dt = 0. \\
	\int_{0}^{1}& \left(f'_y(y + \delta v; \dot{y} + \delta \dot{v}; t)v + f'_{\dot{y}}(y + \delta v; \dot{y} + \delta \dot{v}; t)\dot{v}\right)dt = 0.
\end{align*}

Подставим $\delta = 0$:
\begin{align*}
	\int_{0}^{1}& \left(f'_y(y; \dot{y}; t)v + f'_{\dot{y}}(y; \dot{y};t)\dot{v}\right)dt = 0.
\end{align*}

Проинтегрируем второе слагаемое по частям:
\[
\int_{0}^{1} f'_{\dot{y}}\dot{v}dt = f'_{\dot{y}}\dot{v} \Bigm|^1_0 - \int_{0}^{1} \dfrac{df'_{\dot{y}}}{dt}vdt.
\]

Рассмотрим простую задачу с фиксированными краями (функция должна начинаться и заканчиваться в каких-то точках). Возмущение на краях равно 0, поэтому первое слагаемое равно 0. Тогда получаем:
\begin{align*}
	\int_{0}^{1} \left(f'_y(y; \dot{y}; t)v -  \dfrac{df'_{\dot{y}}}{dt}v\right)dt &= 0. \\
	\int_{0}^{1} \left(f'_y(y; \dot{y}; t) -  \dfrac{df'_{\dot{y}}}{dt}\right)vdt &= 0.
\end{align*}

Так как всё выражение должно быть равно 0 для $\forall \delta$, то выражение в скобках должно быть равно 0. То есть:
\[
f'_y = \dfrac{df'_{\dot{y}}}{dt}.
\]
Это и есть уравнение Эйлера-Лагранжа.

Пример:
\begin{align*}
	&\int_{0}^{\pi} (y^2 - (\dot{y})^2)dt \rightarrow \min_y. \\
	&y(0) = 1, y(\pi) = \dfrac{1}{2}.
\end{align*}
Применяем уравнение Эйлера:
\begin{align*}
	2y = \dfrac{d(-2\dot{y})}{dt} &\Rightarrow y + \ddot{y} = 0. \\
	\lambda^2 + 1 = 0 &\Rightarrow \lambda = \pm i. \\
	y(t) = a\cos &t + b\sin t.
\end{align*}

\section{Разное}
\begin{enumerate}
	\item Гамма-функция:
	
	\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\begin{center}
		
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
	%uncomment if require: \path (0,243); %set diagram left start at 0, and has height of 243
	
	%Shape: Axis 2D [id:dp37706302362235] 
	\draw  (32,200.6) -- (244.75,200.6)(53.28,17) -- (53.28,221) (237.75,195.6) -- (244.75,200.6) -- (237.75,205.6) (48.28,24) -- (53.28,17) -- (58.28,24)  ;
	%Curve Lines [id:da3086431036262438] 
	\draw    (34,175) .. controls (174.75,174) and (206.75,74) .. (219.75,32) ;
	
	
	%Straight Lines [id:da34840091224861813] 
	\draw  [dash pattern={on 0.84pt off 2.51pt}]  (109.75,162) -- (109.75,201) ;
	
	
	%Straight Lines [id:da6799282075689088] 
	\draw  [dash pattern={on 0.84pt off 2.51pt}]  (109.75,162) -- (52.75,163) ;
	
	
	
	% Text Node
	\draw (34,17) node   {$x!$};
	% Text Node
	\draw (238,215) node   {$x$};
	% Text Node
	\draw (110,213) node   {$0.5$};
	% Text Node
	\draw (40,159) node   {$\pi $};
	
	
	\end{tikzpicture}
\end{center}

	\item Задачка на пробит.
	\[
	y_i = \begin{cases}
	1, \text{ if } y_i^* > 0, \\
	0, \text{ if } y_i^* \le 0.
	\end{cases}
	\]
	\[
	y_i^* = \beta_1 + \beta_2x_i + u_i
	\]
	Вопрос: как изменятся коэффициенты, если $u_i$ распределено не $N(0,1)$, а $u_i \sim N(0, 4)$. 
	
	Рассмотрим эту модель:
	\begin{align*}
	y_i^* &= \beta_1 + \beta_2x_i + u_i, \\
	u_i &\sim N(0, 4).
	\end{align*}
	Поделим левую и правую часть на 2:
	\[
	\dfrac{y_i^*}{2} = \dfrac{\beta_1}{2} + \dfrac{\beta_2}{2}x_i + \dfrac{u_i}{2}.
	\]
	Переобозначим:
	\[
	y_i^* = \tilde{\beta_1} + \tilde{\beta_2}x_i + \tilde{u_i}.
	\]
	А это классическая модель с $\tilde{u_i} \sim N(0,1)$. Поэтому ответ: увеличатся в два раза.
	
	 \item Как считать сложные интегралы c параметром. 
	 
	 Пусть есть интеграл:
	 \[
	 \int_{-\infty}^{+\infty}e^{-ax^2}dx = f(a).
	 \]
	 Введём единицы измерения. Пусть $x$ измеряется в слонах. Тогда $dx$ тоже измеряется в слонах. Сделаем так, чтобы $f(a)$ тоже измерялось в слонах. Понятно, что тогда $e^{-ax^2}$ должно измеряться <<ни в чём>>. Чтобы это произошло, $a$ должно измеряться в [слонах$^{-2}$]: тогда в степени экспоненты единицы измерения сократятся. А так как $f(a)$ тоже измеряется в слонах, то $f(a)$ должна быть такой, чтобы [слоны$^{-2}$] переходили в [слоны]. Это означает, что $f(a) = c \dfrac{1}{\sqrt{a}}$. В итоге, восстановили интеграл с точностью до константы.
	 
	 Другой пример:
	 \[
	  \int_{-\infty}^{+\infty}e^{-\frac{x^3}{a}}dx = f(a).
	 \]
	 Рассуждения аналогичные. $a$ должна измеряться в [слонах$^{3}$]. Тогда $f(a)$ должна переводить [слонов$^{3}$] в [слоны]. Тогда $f(a) = c\sqrt[3]{a}$.
\end{enumerate}

\end{document}